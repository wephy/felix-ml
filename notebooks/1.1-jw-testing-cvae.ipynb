{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook tests the performance of a convolutional CVAE. We aim to predict the diffraction pattern when given the structure factors file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.abspath(os.path.join('..')))\n",
    "from src.models.components.cvae import Model\n",
    "\n",
    "data_loc = \"../data/VAE000/cleaned_data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'models/cvae.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[39m=\u001b[39m Model()\n\u001b[0;32m----> 2\u001b[0m model\u001b[39m.\u001b[39mload_state_dict(torch\u001b[39m.\u001b[39;49mload(\u001b[39m\"\u001b[39;49m\u001b[39mmodels/cvae.pt\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n\u001b[1;32m      3\u001b[0m model\u001b[39m.\u001b[39meval()\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/serialization.py:791\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    788\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m pickle_load_args\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    789\u001b[0m     pickle_load_args[\u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m--> 791\u001b[0m \u001b[39mwith\u001b[39;00m _open_file_like(f, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m opened_file:\n\u001b[1;32m    792\u001b[0m     \u001b[39mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    793\u001b[0m         \u001b[39m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    794\u001b[0m         \u001b[39m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    795\u001b[0m         \u001b[39m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    796\u001b[0m         orig_position \u001b[39m=\u001b[39m opened_file\u001b[39m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/serialization.py:271\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    270\u001b[0m     \u001b[39mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 271\u001b[0m         \u001b[39mreturn\u001b[39;00m _open_file(name_or_buffer, mode)\n\u001b[1;32m    272\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    273\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/serialization.py:252\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, name, mode):\n\u001b[0;32m--> 252\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mopen\u001b[39;49m(name, mode))\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'models/cvae.pt'"
     ]
    }
   ],
   "source": [
    "model = Model()\n",
    "model.load_state_dict(torch.load(\"models/cvae.pt\"))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Tensors must have same number of dimensions: got 2 and 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 12\u001b[0m\n\u001b[1;32m      8\u001b[0m structure_factors \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mconcatenate([np\u001b[39m.\u001b[39mloadtxt(\n\u001b[1;32m      9\u001b[0m     os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(data_loc, code, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mcode\u001b[39m}\u001b[39;00m\u001b[39m_structure_factors.txt\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m     10\u001b[0m     dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mfloat64) \u001b[39mfor\u001b[39;00m code \u001b[39min\u001b[39;00m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mchoice(CISD_codes, sample_size)], axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m     11\u001b[0m structure_factors_tensor \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfrom_numpy(structure_factors)\u001b[39m.\u001b[39mfloat()\u001b[39m.\u001b[39mclone()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m predictions_tensor \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mdecode(patterns_tensor, structure_factors_tensor)\n\u001b[1;32m     13\u001b[0m predictions \u001b[39m=\u001b[39m predictions_tensor\u001b[39m.\u001b[39mview(sample_size \u001b[39m*\u001b[39m \u001b[39m128\u001b[39m, \u001b[39m128\u001b[39m)\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy()\n\u001b[1;32m     15\u001b[0m \u001b[39m# Fix images contrasts (due to ZNCC not caring about this)\u001b[39;00m\n",
      "File \u001b[0;32m~/felix-ml/src/models/components/cvae.py:108\u001b[0m, in \u001b[0;36mModel.decode\u001b[0;34m(self, z, conditions)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecode\u001b[39m(\u001b[39mself\u001b[39m, z, conditions):\n\u001b[1;32m    107\u001b[0m     batch_size \u001b[39m=\u001b[39m z\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m)\n\u001b[0;32m--> 108\u001b[0m     z \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder_combine(torch\u001b[39m.\u001b[39;49mcat([z\u001b[39m.\u001b[39;49mview(batch_size, \u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m), conditions], \u001b[39m1\u001b[39;49m))\n\u001b[1;32m    109\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder(z\u001b[39m.\u001b[39mview(batch_size, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m))\n\u001b[1;32m    110\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Tensors must have same number of dimensions: got 2 and 1"
     ]
    }
   ],
   "source": [
    "sample_size = 10\n",
    "CISD_codes = os.listdir(data_loc)\n",
    "\n",
    "patterns = np.concatenate([np.clip(np.fromfile(\n",
    "    os.path.join(data_loc, code, code+\"_+0+0+0.bin\"),\n",
    "    dtype=np.float64), 0.0, 1.0).reshape((128, 128)) for code in np.random.choice(CISD_codes, sample_size)], axis=0)\n",
    "patterns_tensor = torch.from_numpy(patterns).to(torch.float32).clone().detach().view(sample_size, 1, 128, 128)\n",
    "structure_factors = np.concatenate([np.loadtxt(\n",
    "    os.path.join(data_loc, code, f\"{code}_structure_factors.txt\"),\n",
    "    dtype=np.float64) for code in np.random.choice(CISD_codes, sample_size)], axis=0)\n",
    "structure_factors_tensor = torch.from_numpy(structure_factors).float().clone().detach().view(sample_size, -1)\n",
    "predictions_tensor = model.decode(torch., structure_factors_tensor)\n",
    "predictions = predictions_tensor.view(sample_size * 128, 128).detach().numpy()\n",
    "\n",
    "# Fix images contrasts (due to ZNCC not caring about this)\n",
    "patterns = patterns - np.min(patterns)\n",
    "patterns = patterns * (1 / np.max(patterns))\n",
    "predictions = predictions - np.min(predictions)\n",
    "predictions = predictions * (1 / np.max(predictions))\n",
    "difference = np.abs(predictions - patterns)\n",
    "difference = np.clip(5 * difference, 0.0, 1.0)\n",
    "\n",
    "fig = plt.figure(figsize=(25, 4))\n",
    "gs = gridspec.GridSpec(3, 1, height_ratios=[1, 1, 1],\n",
    "         wspace=0.0, hspace=0.0, top=0.95, bottom=0.05, left=0.17, right=0.845) \n",
    "\n",
    "for i, (ax, img) in enumerate(zip([plt.subplot(gs[0, 0]), plt.subplot(gs[1, 0]), plt.subplot(gs[2, 0])],\n",
    "                   [patterns.T, predictions.T, difference.T])):\n",
    "    ax.imshow(img, cmap=(\"CMRmap\" if i==2 else \"gray\"))\n",
    "    ax.set_axis_off()\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
